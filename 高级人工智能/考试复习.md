# 高级人工智能
## CNN
卷积神经网络是一类包含卷积计算且具有深度结构的<mark style="background: #FF5582A6;">前馈神经网络</mark>，是深度学习的代表算法之一。卷积神经网络具有<mark style="background: #FF5582A6;">表征学习能力</mark>，能够按其阶层结构对输入信息进行平移不变分类，因此也被称为“平移不变人工神经网络”。
### CNN包括：
1. 卷积层：卷积层是CNN的核心，通过在输入图像上<mark style="background: #FF5582A6;">滑动卷积核</mark>来进行卷积运算，并生成特征图
2. 激活函数：激活函数用于<mark style="background: #FF5582A6;">引入非线性</mark>，以便CNN能够捕获更复杂的模式。常见的激活函数包括ReLU、Sigmoid和Tanh等
3. 池化层：池化层用于<mark style="background: #FF5582A6;">减少图的维度，同时保留重要的特征</mark>
4. 同时保留重要的特征。常用的池化操作包括最大池化和平均池化
5. 全连接层：全连接层用于将<mark style="background: #FF5582A6;">卷积层和池化层</mark>输出连接起来，并生成分类或回归结果
### 卷积神经网络训练技巧
1. 多个小卷积核代替大卷积核
	虽然小卷积核的感受野小于大卷积核，但通过堆叠多层小卷积核可以达到与大卷积核一致的感受野， 这样有两个好处，一方面需要的参数个数更少，计算复杂度降低了，另一方面多层小卷积核有了更多层的激活函数，使得分类器的非线性更好。
	感受野：特征图中一个网格点所对应的原始图像中的图像区域大小
2. 数据预处理
	数输入数据可以成倍增长，尤其对于深度学习这类需要大量训练数据的算法，数据预处理很重要
	预处理的方法有：
	1. 水平翻转
	2. 随机裁剪/大小变换
	3. 角度变换，拉伸，修剪等
3. Transfer Learning
	虽然已经做了数据增强，但需要的数据量还是不够，这时候就需要别人训练好的网络，借鉴别人的参数，在此基础上训练自己的网络。
### CNN的创新之处
1. 共享权重：CNN通过卷积核共享权重，减少了需要训练的参数数量，从而网络更加简洁、高效
2. 局部感受野：CNN采用卷积操作来提取图像中的局部特征，这种局部感受野可以使网络更好地捕获空间结构信息
3. 池化层：CNN通过池化层来减少特征图的维度
### CNN的技术细节包括
1. 卷积核的大小、数量和步长可以根据需要进行调整，以获得最佳的特征提取效果
2. CNN的损失函数通常采用交叉熵损失，用于评估分类任务的性能
3. CNN的优化器通常采用随机梯度下降或其变体，以最小化损失函数
4. CNN通常需要大量的数据训练来获良好的性能，因此数据增强技术也很重要，如旋转、反转和裁剪等
## RNN
循环神经网络是一类以序列数据为输入，在序列的演进方向进行递归且所有节点按链式连接的递归神经网络。RNN跟传统神经网络最大的区别就是每次都会将前一次的输出结果，带到下一次的隐藏层中，一起训练。
### 优点：
1. 对具有序列特性的数据非常有效，他能挖掘数据中的时序信息以及语义信息。在自然语言处理如：语音识别、语言建模、机器翻译等领域皆有应用。
2. 无论输入多少个单词，它都能给出相应的输出。这就很好的解决了不定长输入的问题。
3. 它可以将先前的信息链接到当前的任务。
### 缺点：
1. RNN在误差梯度经过多个时间步的反向传播后容易导致极端的非线性行为，包括<mark style="background: #FF5582A6;">梯度消失</mark>和<mark style="background: #FF5582A6;">梯度爆炸</mark>
2. 短期的记忆影响较大，长期的记忆影响很小
3. 递归计算速度慢
### 改进
1. 梯度阶段
2. 正则化
3. 层归一化
### LSTM
#### 优点
1. 引入<mark style="background: #FF5582A6;">记忆单元</mark>和<mark style="background: #FF5582A6;">门控机制</mark>，有效解决RNN中的长期依赖问题
2. 具有记忆功能
3. 可处理变长序列
#### 缺点
1. 计算量大
2. 模型复杂度高
3. 需要更多的数据以避免过拟合
### GRU
#### 优点
GRU少了一个门控单元，因此计算复杂度比LSTM低，同时在某些情况下也能和SLTM达到类似的性能。此外，GRU解减少梯度小事的问题
#### 缺点
1. 某些复杂序列数据，GRU的长期依赖能力可能不如LSTM
2. 一些特殊情况下，GRU可能会出现梯度消失或梯度爆炸的问题
### CONVLSTM
针对视频这种包含时间序列和空间信息的数据提出的。
## GAN
一个网络（生成模型）生成模拟数据，另一个网络（判别模型）判断生成的数据是真实的还是模拟的。生成网络要不断优化自己生成的数据让别的网络判断不出来，判别网络也要优化自己让自己的判断的更准确。二者形成对抗，因此叫对抗神经网络。
### 优点
1. 生成高质量、逼真的虚假样本
2. 无需对训练数据进行显示建模
### 缺点
1. 训练困难
2. 生成的图象是随机的，不可预测的，无法控制网络输出特定的图片，生成目标不明确，可控性不强。
3. 可能模式塌陷问题，生成器只能生成某一种类型的样本，无法生成多样本的样本
## FCN
### CNN的局限性
池化层会使卷积神经网络的高层特征映射的分辨率远小于原始图像，损失了较多的空间位置.
### 优点
1. 端对端输出
2. 高效率
### 3. 缺点
1. 小目标分割效果差
2. 分割边界粗糙
3. 过分割和欠分割问题突出
### 注意力机制
#### 优点
1. 参数少：相比于CNN、RNN，其复杂度更小，参数也更少。所以对算力的要求也就更小。
2. 速度快：Attention解决了RNN及其变体模型不能并行计算的问题。注意力机制每一步计算不依赖于上一步的计算结果，因此可以和CNN一样并行处理
3. 效果好：在注意力机制引入之前，有一个问题大家一直很苦恼：长距离的信息会被弱化，就好像记忆能力弱的人，记不住过去的事情是一样的
#### 缺点
1. 计算复杂度高，计算每个输入与其他向量之前的相似度，指数增长
2. 噪声敏感，噪声也会有权重，影响模型效果
3. 需要的数据量大，要大量学习
4. 模型解释性差，内部结构复杂，难以解释
## RNN预测暴雨
1. 数据采集
	收集历史的天气信息，包括雨量、温度 、湿度、气压等信息
2. 数据预处理
	将收集到的数据进行处理，例如填充缺失值、去除异常值等。
	同时，需要将数据按照一定的时间段/顺序划分为**训练集**和**测试集**
3. 特征提取
	对数据特征进行特征提取，以便用于RNN模型的输入。
4. 构建RNN模型，选择适合的RNN模型，并进行网络结构设计和参数调整。常用的RNN模型包括RNN、LSTM和GRU等
5. 训练模型
6. 模型评估
	使用测试数据对模型进行评估，评估指标包括均方误差和平均绝对误差
7. 优化模型
8. 预测未来暴雨
	使用训练好的RNN模型预测未来暴雨
## RNN翻译
1. 数据预处理
	将文本转换为数字化表示
2. 构建RNN模型
	常用的包括基于LSTM或GRU的解码器-编码器模型
	- 构建编码器：使用RNN构建编码器，将源语言文本输入到编码器中
	- 提取上下文向量：使用编码器的最后一个隐藏状态作为上下文向量，也可以使用注意力机制从编码其中所有隐藏状态中提取上下文向量
	- 构建解码器：使用RNN构建解码器
	- 计算损失函数
3. 训练模型：使用训练集进行训练
4. 评估模型
5. 优化模型
## CNN图像分类
1. 收集数据集
	将数据集分为训练集、验证集和测试集
2. 构建模型：
	设计卷积神经网络模型，包括卷积层、池化层和全连接层。其中卷积层提取特征，池化层降低特征图尺寸，全连接层将特征映射到输出空间。
3. 训练模型
4. 评估模型
5. 优化模型
## CNN手写数字识别
1. 数据收集
2. 数据预处理
3. 构建CNN模型
	1. 定义输入层
	2. 定义卷积层
	3. 定义池化层
	4. 定义全连接层
	5. 定义输出层
4. 训练CNN模型
5. 评估CNN模型
6. 优化模型